import os
import streamlit as st
from langchain_openai import OpenAIEmbeddings, OpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dotenv import load_dotenv
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = "gpt-4o-mini"
FAQ_URL = "https://ti.ua/ua/faq/"
STORES_URL = "https://ti.ua/ua/nashi-magazini/"

FAQ_KEYWORDS = ["–∫—Ä–µ–¥–∏—Ç", "—Ä–æ–∑—Å—Ç—Ä–æ—á–∫–∞", "–ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–ø–ª–∞—Ç–∞", "–≥–∞—Ä–∞–Ω—Ç—ñ—è"]

FAQ_CLASSES = ["faq-accordeon", "text-content"]
FAQ_ATTRIBUTES = ["data-toggle", "role", "aria-expanded", "data-bs-toggle", "data-target", "data-bs-target"]


def setup_chrome_driver():
    """Setup Chrome driver for web scraping"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
    
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)


def extract_text_from_elements(soup, selectors):
    """Extract text from elements using selectors"""
    texts = []
    for selector in selectors:
        elements = soup.find_all(**selector)
        for element in elements:
            text = element.get_text(separator="\n", strip=True)
            if len(text) > 20:
                texts.append(text)
    return texts


def extract_faq_content(soup):
    """Extract FAQ content from page"""
    all_texts = []
    
    for class_name in FAQ_CLASSES:
        elements = soup.find_all(class_=class_name)
        for element in elements:
            all_texts.append(element.get_text(separator="\n", strip=True))
    
    for attr in FAQ_ATTRIBUTES:
        elements = soup.find_all(attrs={attr: True})
        for element in elements:
            text = element.get_text(separator="\n", strip=True)
            if len(text) > 20:
                all_texts.append(text)
    
    for element in soup.find_all(id=True):
        element_id = element.get("id", "").lower()
        if any(keyword in element_id for keyword in ["faq", "accord", "collapse"]):
            text = element.get_text(separator="\n", strip=True)
            if len(text) > 20:
                all_texts.append(text)
    
    for element in soup.find_all():
        text = element.get_text(strip=True)
        if len(text) > 50 and any(keyword in text.lower() for keyword in FAQ_KEYWORDS):
            all_texts.append(text)
    
    return list(set(all_texts))


def fetch_page_text(url):
    """Load and parse text from web page"""
    driver = None
    try:
        driver = setup_chrome_driver()
        driver.get(url)
        time.sleep(3)
        
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, "html.parser")
        
        for tag in soup(["script", "style"]):
            tag.decompose()
        
        if "faq" in url.lower():
            faq_texts = extract_faq_content(soup)
            if faq_texts:
                return "\n\n".join(faq_texts)
        
        text = soup.get_text(separator="\n")
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        return "\n".join(lines)
        
    except Exception as e:
        st.error(f"Error parsing {url}: {str(e)}")
        return ""
    finally:
        if driver:
            driver.quit()


def prepare_corpus():
    """Prepare corpus data from web pages"""
    faq_text = fetch_page_text(FAQ_URL)
    stores_text = fetch_page_text(STORES_URL)
    
    return [
        {"source": "FAQ", "text": faq_text},
        {"source": "Stores", "text": stores_text},
    ]


def build_vectorstore(docs):
    """Create vector store from documents"""
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = []
    metadatas = []
    
    for doc in docs:
        for chunk in splitter.split_text(doc["text"]):
            texts.append(chunk)
            metadatas.append({"source": doc["source"]})
    
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_texts(texts, embeddings, metadatas=metadatas)
    return vectorstore


def create_qa_prompt():
    """Create prompt for QA system"""
    return PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é –¢–Ü–õ–¨–ö–ò –Ω–∞ –∑–∞–¥–∞–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –Ω–∞–¥–∞–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ FAQ TI.UA.\n"
            "–ù–ï –≥–µ–Ω–µ—Ä—É–π –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ. –î–∞–≤–∞–π —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å.\n"
            "–í–ê–ñ–õ–ò–í–û: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –í–°–Æ –¥–æ—Å—Ç—É–ø–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. –ù–µ –ø—Ä–æ–ø—É—Å–∫–∞–π –¥–µ—Ç–∞–ª—ñ.\n"
            "–Ø–∫—â–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ —î –¥–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∫—Ä–µ–¥–∏—Ç–∏, —Ä–æ–∑—Å—Ç—Ä–æ—á–∫—É, —Å–µ—Ä–≤—ñ—Å–∏ - –æ–±–æ–≤'—è–∑–∫–æ–≤–æ –≤–∫–ª—é—á–∏ —ó—Ö —É –≤—ñ–¥–ø–æ–≤—ñ–¥—å.\n"
            "–ù–ï –∫–∞–∂–∏ '—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –≤—ñ–¥—Å—É—Ç–Ω—è' –∞–±–æ '–∑–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ —ñ–Ω—à–∏—Ö –¥–∂–µ—Ä–µ–ª'. –Ø–∫—â–æ —Ç–æ—á–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ–º–∞—î, —Å–∫–∞–∂–∏ '–ü–∏—Ç–∞—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞'.\n"
            "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n"
            "–ü–∏—Ç–∞–Ω–Ω—è: {question}\n"
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        )
    )


def get_qa_chain(vectorstore):
    """Create QA chain"""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model=OPENAI_MODEL)
    prompt = create_qa_prompt()
    
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    return qa


def process_date_query(query):
    """Process date queries"""
    today = datetime.now().strftime("%d.%m.%Y")
    tomorrow = (datetime.now() + timedelta(days=1)).strftime("%d.%m.%Y")
    return query.replace("—Å—å–æ–≥–æ–¥–Ω—ñ", today).replace("–∑–∞–≤—Ç—Ä–∞", tomorrow)


def initialize_app():
    """Initialize application"""
    if "vectorstore" not in st.session_state:
        with st.spinner("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö..."):
            docs = prepare_corpus()
            st.session_state.vectorstore = build_vectorstore(docs)
            st.session_state.qa = get_qa_chain(st.session_state.vectorstore)
    
    if "history" not in st.session_state:
        st.session_state.history = []


def main():
    """Main application function"""
    st.set_page_config(page_title="–ß–∞—Ç –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ TI.UA", page_icon="üí¨")
    st.title("üí¨ –ß–∞—Ç –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ TI.UA")
    
    initialize_app()
    
    user_input = st.text_input("–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:", key="user_input")
    
    if st.button("–í—ñ–¥–ø—Ä–∞–≤–∏—Ç–∏") and user_input:
        query = process_date_query(user_input)
        qa = st.session_state.qa
        result = qa.invoke({"query": query})
        answer = result["result"].strip()
        
        if len(answer) < 10 or "–Ω–µ –∑–Ω–∞—é" in answer.lower() or "–Ω–µ –º–æ–∂—É" in answer.lower():
            answer = "–ü–∏—Ç–∞—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞"
        
        st.session_state.history.append((user_input, answer))
    
    for q, a in st.session_state.history[::-1]:
        st.markdown(f"**–í–∏:** {q}")
        st.markdown(f"**–ë–æ—Ç:** {a}")


if __name__ == "__main__":
    main()
